<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Emily Schultz" />

<meta name="date" content="2023-04-23" />

<title>ESS 3500 R Handbook, Lesson 10</title>

<script src="site_libs/header-attrs-2.30/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ESS 3500 R Handbook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lessons
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RHandbook_1.html">Lesson 1: Software installation</a>
    </li>
    <li>
      <a href="RHandbook_2.html">Lesson 2: Data management</a>
    </li>
    <li>
      <a href="RHandbook_3.html">Lesson 3: R basics</a>
    </li>
    <li>
      <a href="RHandbook_4.html">Lesson 4: Data frames</a>
    </li>
    <li>
      <a href="RHandbook_5.html">Lesson 5: Data visualization</a>
    </li>
    <li>
      <a href="RHandbook_6.html">Lesson 6: Graphing in R</a>
    </li>
    <li>
      <a href="RHandbook_7.html">Lesson 7: Hypothesis testing</a>
    </li>
    <li>
      <a href="RHandbook_8.html">Lesson 8: Probability</a>
    </li>
    <li>
      <a href="RHandbook_9.html">Lesson 9: Probability distributions</a>
    </li>
    <li>
      <a href="RHandbook_10.html">Lesson 10: Chi-square tests</a>
    </li>
    <li>
      <a href="RHandbook_11.html">Lesson 11: T-tests</a>
    </li>
    <li>
      <a href="RHandbook_12.html">Lesson 12: Assumptions and transformations</a>
    </li>
    <li>
      <a href="RHandbook_13.html">Lesson 13: ANOVA</a>
    </li>
    <li>
      <a href="RHandbook_14.html">Lesson 14: Correlations and Regressions</a>
    </li>
    <li>
      <a href="RHandbook_15.html">Lesson 15: Multiple predictors</a>
    </li>
  </ul>
</li>
<li>
  <a href="RHandbook_templates.html">R Templates</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">ESS 3500 R Handbook, Lesson 10</h1>
<h4 class="author">Emily Schultz</h4>
<h4 class="date">2023-04-23</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#lesson-10-categorical-data"
id="toc-lesson-10-categorical-data">Lesson 10: Categorical Data</a>
<ul>
<li><a href="#conceptural-overview-of-chi-square-tests"
id="toc-conceptural-overview-of-chi-square-tests">10.1 Conceptural
overview of Chi-square tests</a>
<ul>
<li><a href="#goodness-of-fit-test"
id="toc-goodness-of-fit-test">Goodness of fit test</a></li>
<li><a href="#test-of-independence" id="toc-test-of-independence">Test
of independence</a></li>
<li><a href="#the-chi-square-test-statistic"
id="toc-the-chi-square-test-statistic">The Chi-square test
statistic</a></li>
<li><a href="#the-chi-square-distribution"
id="toc-the-chi-square-distribution">The Chi-square
distribution</a></li>
<li><a href="#drawing-conclusions" id="toc-drawing-conclusions">Drawing
conclusions</a></li>
</ul></li>
<li><a href="#chi-square-tests-in-r" id="toc-chi-square-tests-in-r">10.2
Chi-square tests in R</a>
<ul>
<li><a href="#goodness-of-fit" id="toc-goodness-of-fit">Goodness of
fit</a></li>
<li><a href="#test-of-independence-1"
id="toc-test-of-independence-1">Test of independence</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="lesson-10-categorical-data" class="section level1">
<h1>Lesson 10: Categorical Data</h1>
<p>In our first foray into statistical testing, we will focus on
Chi-square analyses. Like when we decide on a type of graph to make, the
types of variables we are working with in our analysis help us determine
the type of statistical test to use. Chi-square analysis are used when
we have a categorical independent variable and count data (discrete) as
the dependent variable. In this lesson, we will start with a conceptual
overview of Chi-square tests and then learn how to run the tests in
R.</p>
<div id="conceptural-overview-of-chi-square-tests"
class="section level2">
<h2>10.1 Conceptural overview of Chi-square tests</h2>
<p>Chi-square tests are used to compare the frequencies of observations
in different categories to expected frequencies for those categories.
There are two common versions of this test: the goodness of fit test and
the test of independence (contingency table test). The goodness of fit
test is used when you have a single categorical variable and you want to
compare the proportion of observations in each category to an expected
proportion. The test of independence is used when you have two
categorical variables and you want to test whether the frequency of
observations in the categories of one variable depend on the other
variable. Both tests are described in more detail below. The overall
approach is similar for both, though, and is the same general approach
as we discussed in our lesson on hypothesis testing. You start by
identifying expected values based on a null hypothesis, calculate
deviations between your expected and observed frequencies, and calculate
the probability of deviation greater than or equal to those you
observed, assuming the null hypothesis is true.</p>
<div id="goodness-of-fit-test" class="section level3">
<h3>Goodness of fit test</h3>
<p>The Chi-square goodness of fit test compares observed frequencies of
categories for a single variable to expected observations for those
categories. Some examples of questions that can be answered with a
goodness of fit test are:</p>
<ol style="list-style-type: decimal">
<li>Does the frequency of parasite infection vary between two species of
antelope?</li>
<li>Do the genotype frequencies in a population of fish differ from the
expectations of the Hardy-Weinberg equilibrium model?</li>
<li>Does the frequency of traits in offspring differ from the prediction
of Mendelian inheritance?</li>
</ol>
<div id="null-and-alternative-hypotheses" class="section level4">
<h4>Null and alternative hypotheses</h4>
<p>The null hypothesis for a Chi-square goodness of fit test is that the
frequency of observations in equal to the expected frequency, and the
alternative hypothesis is that they are not equal. The expected
frequency depends on our question. Oftentimes, we simply want to ask if
the frequency of observations is equal amongst our categories. In that
case, our null hypothesis would be that the frequencies, or proportions,
are equal. However, in other cases, we might have a different
expectation. Genetics is a good example of this. In studies of
inheritance, we are often interesting in determining if a trait follows
Mendelian inheritance patterns. In this case, our null hypothesis would
be based on the expectations of Mendelian inheritance. For example, if
we crossed two heterozygous parents, we would expect 3/4 of the
offspring to have the dominant trait and 1/4 of offspring to have the
recessive trait. These would then be the expected frequencies for our
null hypothesis.</p>
</div>
</div>
<div id="test-of-independence" class="section level3">
<h3>Test of independence</h3>
<p>The Chi-square test of independence, also known as a contingency
table test, is used when you have two categorical variables, and you
want to test if the frequency of observations in one variable is
influenced by the other variable. Some examples of questions that can be
answered with a test of independence are:</p>
<ol style="list-style-type: decimal">
<li>Does the frequency of bee visits to different flower colors depends
on the species of bee?</li>
<li>Does the frequency of malaria infection depend on an individual’s
genotype?</li>
<li>Does the habitat type affect the frequency of the presence/absence
of an endangered plant?</li>
</ol>
<div id="null-and-alternative-hypotheses-1" class="section level4">
<h4>Null and alternative hypotheses</h4>
<p>Like in a goodness of fit test, in a test of independence, the null
hypothesis is that the frequency of observations in equal to the
expected frequency, and the alternative hypothesis is that they are not
equal. What differs is how we determine the expected frequencies. In a
goodness of fit test, we have some pre-determined expectation of
frequencies for our null hypothesis. In a test of independence, we are
interested in the relationship between our variables, and the expected
frequencies are based on the values of the observations in our data set.
Let’s look at one of our example questions from above to see how we
would determine the expected values.</p>
<p>Image you are trying to test whether different bee species have
different preferences for flower colors. Here you have two different
categorical variables: species and flower color. You want to test if the
frequency of bee visits to different flower colors depends on the
species of bee. The data you collect might look something like this:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Yellow</th>
<th align="center">Purple</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Species 1</td>
<td align="center">10</td>
<td align="center">25</td>
<td align="center">35</td>
</tr>
<tr class="even">
<td align="left">Species 2</td>
<td align="center">30</td>
<td align="center">20</td>
<td align="center">50</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="center">40</td>
<td align="center">45</td>
<td align="center">85</td>
</tr>
</tbody>
</table>
<p>These are the observed values. To determine our null hypothesis, we
need to calculate the expected frequencies for each pair of categories.
In order to do this, we can use the rule of multiplication that we
learned about when we covered probability. The logic here is that if the
preferred flower color is independent of the bee species, the
probability that a bee from species 1 visits a yellow flower should be
equal to the total probability that a bee belongs to species 1
multiplied by the total probability of a visit to a yellow flower
(recall that the multiplication rule only works like this for
independent events - when the outcome of one event does not affect the
probability of the other). In this example we observed that 35 of the 85
bees were from species 1 and that 40 of the 85 visits were to yellow
flowers, so the probability of a bee of species one visiting a yellow
flower is <span class="math inline">\(\frac{35}{85} \times \frac{40}{85}
= 0.194\)</span>. We can then calculate the expected frequency by
multiplying this probability by the totat number of observed
individuals: <span class="math inline">\(0.194 \times 85 =
16.5\)</span>. We can then use this same procedure to calculate the
expected values for the remaining categories:</p>
<table>
<colgroup>
<col width="16%" />
<col width="27%" />
<col width="27%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Yellow</th>
<th align="center">Purple</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Species 1</td>
<td align="center"><span class="math inline">\(\frac{35}{85} \times
\frac{40}{85} \times 85 = 16.5\)</span></td>
<td align="center"><span class="math inline">\(\frac{35}{85} \times
\frac{45}{85} \times 85 = 18.5\)</span></td>
<td align="center">35</td>
</tr>
<tr class="even">
<td align="left">Species 2</td>
<td align="center"><span class="math inline">\(\frac{50}{85} \times
\frac{40}{85} \times 85 = 23.5\)</span></td>
<td align="center"><span class="math inline">\(\frac{50}{85} \times
\frac{45}{85} \times 85 = 26.5\)</span></td>
<td align="center">50</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="center">40</td>
<td align="center">45</td>
<td align="center">85</td>
</tr>
</tbody>
</table>
<p>These values provide us with the expectation for our null hypothesis,
and our alternative hypothesis would be that the observed frequencies do
not match these expectations.</p>
</div>
</div>
<div id="the-chi-square-test-statistic" class="section level3">
<h3>The Chi-square test statistic</h3>
<p>After identifying the expected frequencies and hypothesis, the
process for hypothesis testing is similar for the goodness of fit test
and the test of independence. Unlike in the previous hypothesis testing
we practiced, when we run a Chi-square test, we don’t directly calculate
the probability of our observed values. Instead we calculate a test
statistic that summarized the magnitude of the difference between our
observed an expected values. This test statistic is called the
Chi-square statistic. The larger the Chi-square value, the greater the
difference between the observed values and the expected values based on
the null hypothesis. We then calculate the probability of getting a
Chi-square value as large or larger than our Chi-square value, assuming
the null hypothesis is true.</p>
<p>To calculate the Chi-square value for either a goodness of fit test
or a test of independence, you first calculate the difference between
the observed and expected values for each category.Then you square those
values, which will make all of the deviations positive, provided a
measure of the magnitude of the differences, regardless of the direction
of the differences for each category. Finally, you divide the squared
deviations by the expected value for each category and sum the results.
Here is the full equation:</p>
<p><span class="math display">\[
\sum{\frac{(O-E)^2}{E}},
\]</span> where <span class="math inline">\(O\)</span> is the observed
value and <span class="math inline">\(E\)</span> is the expected value.
In this class, you will mostly be using R to do this calculation, but it
is important to keep in mind that the Chi-square statistic is a measure
of the overall difference between observed and expected values in the
data set.</p>
</div>
<div id="the-chi-square-distribution" class="section level3">
<h3>The Chi-square distribution</h3>
<p>Once we have a Chi-square value, we can use it to calculate the
p-value: the probability of a Chi-square value as large or larger than
ours (in other words, a difference between the observed and expected
values as large or larger than what we observed), assuming the null
hypothesis is true. We do this using a probability distribution that
gives us the probability of a certain Chi-square value, based on the
null hypothesis. This probability distribution is the Chi-square
distribution.</p>
<p>The figure below is an example of a Chi-square distribution. The
shape is intuitive, based on what we would expect from our null
hypothesis. If the null hypothesis is true, that mean our observed
values should be a close match for the expected values, and it is
therefore more probable that the Chi-square value is low. It is less
probable that you will get a large Chi-square value if the null
hypothesis is true.</p>
<div class="float">
<img src="ChiDist2df.png" style="width:60.0%"
alt="A Chi-square distribution for 2 degrees of freedom. There is a high probability of Chi-square values close to zero and a low probability of Chi-square values far from zero." />
<div class="figcaption">A Chi-square distribution for 2 degrees of
freedom. There is a high probability of Chi-square values close to zero
and a low probability of Chi-square values far from zero.</div>
</div>
<p>The specific shape of the Chi-square distribution depends on a
parameter known as the degrees of freedom. The degrees of freedom are a
measure of the amount of independent information in your data. In many
contexts, it is related to the sample size of your data. In Chi-square
tests it is related to the number of categories in your data.</p>
<p>For illustration, let’s first consider a goodness of fit test, where
you are comparing the frequencies between two categories. You have some
total number of observations, say 100, and those 100 observations will
be split between your two categories. Given the constraint of 100 total
observations, the number of observations in the first category could be
any value from 0 to 100. But once the number of observations in the
first category has been set, there is only one possible value for the
number of observations in your second category that will lead to a total
of 100. For example, if there were 40 observations in the first
category, there would have to be 60 in the second. This means only one
of the values in the data set is “free to vary” and the degrees of
freedom for these data are 1. If you had three categories instead of
just two, then the values for two of those categories could vary, and
third could not, so the degrees of freedom would be 2. In general, for a
goodness of fit test, the degrees of freedom is equal to the number of
categories minus 1.</p>
<p>For a test of independence, in addition to calculating the total
number of observations, we calculate the totals for each individual
category. In our bee and flower table above, for example, we calculated
the total number of observations of species 1 and the total number of
observations on yellow flowers. This puts further constraints on the
freedom of our data, so even though there are four distinct combinations
of categories in that example, the degrees of freedom is only one. If we
set the value for just one category, we would be able to definitely
calculate the values for the other categories, based on that one value
along with the totals in the table. The general formula for degrees of
freedom in a test of independence is <span
class="math inline">\((r-1)(c-1)\)</span>, where <span
class="math inline">\(r\)</span> and <span
class="math inline">\(c\)</span> refer to the number of categories in
the rows and columns of our table, respectively.</p>
<p>The degrees of freedom in our data affects the shape of the
Chi-square distribution we would use to draw conclusions about our data.
The figure below shows how the shape of a Chi-square distribution varies
for four different values of degrees of freedom.</p>
<p><img src="ChiDist.png" style="width:60.0%"
alt="Chi-square distributions with degrees of freedom of 1, 2, 5, and 10. As the degrees of freedom increases, the distribution gets pulled to the right." />
The effect of the degrees of freedom on the shape of the distribution is
again intuitive. The degrees of freedom will increase with the number of
categories in our data set. When we calculate the Chi-square value, we
sum up the deviations between the observed and expected values across
all of our categories, so if we have more categories, we would expect
the Chi-square value to be larger. It therefore makes sense that, even
if the null hypothesis is true, there is still a higher probability of a
larger Chi-square value, even if our null hypothesis is true.</p>
</div>
<div id="drawing-conclusions" class="section level3">
<h3>Drawing conclusions</h3>
<p>Once we have calculated our Chi-square value and identified the
degrees of freedom in our data set, we can use the correct Chi-square
distribution to draw the final conclusion about our data. Again, we want
to use the distribution to calculate the probability of a Chi-square
value greater than or equal to ours, assuming the null hypothesis is
true. This will be our p-value.</p>
<p>That values can be calculated from the Chi-square distribution by
calculating the area under the curve of the distribution for values
greater than or equal to your Chi-square value. The graph below shows an
example of what that would look like for a Chi-square value of 3 from a
data set with 2 degrees of freedom. The purple shading represents the
probability of a Chi-square value greater than or equal to 3.</p>
<p><img src="ChiP.png" style="width:60.0%"
alt="A Chi-square distribution with 2 degrees of freedom, showing the area that represents the probability of a Chi-square value greater than or equal to 3." />
The value of the area shown (the p-value) is 0.2231, meaning that there
is an approximately 22% chance of getting a Chi-square value of 3 or
greater, assuming the null hypothesis is true. We interpret this the
same way we interpreted the p-values in our hypothesis testing examples.
If the p-value is &lt;0.05, we reject the null hypothesis. In this case,
we would not reject the null hypothesis.</p>
<p>Moving forward, we will be using R to run Chi-square tests. You will
not be expected to be doing all of these calculations by hand. However,
understanding the conceptual underpinning of the tests is important for
correctly implementing and interpreting the test results.</p>
</div>
</div>
<div id="chi-square-tests-in-r" class="section level2">
<h2>10.2 Chi-square tests in R</h2>
<p>For this lesson, we will be working with data collected by Dr. Alan
Garfinkel, who was researching methods for predicting the risk of heart
attacks in older patients. Younger patients are often tested for heart
attack risk by first exercising and then having their heart stress
measured with an ECG. However, older patients are often unable to
tolerate the exercise required for the test. Dr. Garfinkel was testing
whether a medication called dobutamine, which mildly stresses the heart,
can be used as an alternative. He gave patients a heart stress test
after they took dobutamine, and then he followed the patients for a year
to see if they had a heart attack within that time frame. This allowed
him to test whether a positive result on the dobutamine stress test was
a good predictor of heart attack risk.</p>
<div id="goodness-of-fit" class="section level3">
<h3>Goodness of fit</h3>
<p>For our first test, we will run what is called a goodness of fit
test. We can use this type of test to determine if the observed
frequency of samples in different categories follow some expected
frequency (the expected frequencies are the null hypothesis). In this
example, we will work with the cardiac data and test whether male and
female patients were equally represented in the study. In other words,
was there a 50:50 ratio of male and female patients.</p>
<p>First, load the data set. Be sure your working directory is set to
the location of the cardiac data file.</p>
<pre class="r"><code>cardiac &lt;- read.csv(&quot;cardiac.csv&quot;)</code></pre>
<p>Now, make a simple bar graph showing the frequency of male and female
patients in the sample.</p>
<pre class="r"><code>library(ggplot2)
ggplot(cardiac, aes(x=gender)) +
  geom_bar() +
  labs(x=&quot;Gender&quot;, y=&quot;Frequency&quot;) +
  theme_classic()</code></pre>
<p><img src="RHandbook_10_files/figure-html/gender_bar-1.png" alt="" width="672" /></p>
<p>Before we can run our Chi-square test to determine if males and
females were equally represented, we have to summarize our data. The
Chi-square test function in R requires a simple table with the count of
samples in each category (the number of male and female patients, in
this example). We can create this table easily in R using the
<code>table</code> function. The argument for this function is the
variable we want to summarize, in this case, the “gender” variable from
our cardiac data frame. Sometimes it can also be helpful to see the
proportions instead of the counts. We can convert our table into
proportions using the <code>prop.table</code> function.</p>
<pre class="r"><code>gender_table &lt;- table(cardiac$gender)
gender_prop &lt;- prop.table(gender_table)</code></pre>
<p>Now that we have our table with the counts of the two genders, we are
ready to run our test. We will use the <code>chisq.test</code> function.
The first arguments is the table with the counts, that we just created.
The second argument is a vector with the expected proportion for each
category. Because we are testing whether the genders are
equally-represented, our expected frequencies (i.e., our null
hypothesis) are 0.5 and 0.5.</p>
<pre class="r"><code>gender_test &lt;- chisq.test(gender_table, p=c(0.5,0.5))
gender_test</code></pre>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  gender_table
## X-squared = 24.953, df = 1, p-value = 5.873e-07</code></pre>
<p>Based on the output, were male and female patients equally
represented in the study?</p>
<p>If they are not equally represented, it is also important to
determine which category is over-represented and which is
under-represented. It is pretty easy in this case because we only have
two categories, and we were testing if they had equal proportions. In
other cases, the math might not be quite as simple, but we can get R to
help us out. We can look at some additional output from the test to see
the observed and expected values for each category as follows.</p>
<pre class="r"><code>gender_test$observed</code></pre>
<pre><code>## 
## female   male 
##    338    220</code></pre>
<pre class="r"><code>gender_test$expected</code></pre>
<pre><code>## female   male 
##    279    279</code></pre>
<p>Which gender was over-represented, and which gender was
under-represented?</p>
</div>
<div id="test-of-independence-1" class="section level3">
<h3>Test of independence</h3>
<p>For the next test, we will work with a different type of question.
Instead of just comparing the frequency in different categories of one
variable, we will work with two sets of categories and test whether the
frequencies of one categorical variable are affected by the other
categorical variable. In this case, we will use the same cardiac data
set to test whether patients who tested positive for heart stress on an
ECG after receiving dobutamine were more likely to have a myocardial
infarction (heart attack) within the next year than patients who tested
negative for heart stress.</p>
<p>We’ll again begin by making a bar graph. This time, because we are
looking at the frequencies in two different categories, we will make a
stacked bar graph. We will use the ECG results as our x variable, and
each bar the ECG result categories with have two colors, showing the
proportion of patients in each of those categories that did or did not
have a heart attack.</p>
<pre class="r"><code>ggplot(cardiac, aes(x=posECG,fill=newMI)) +
  geom_bar() +
  labs(x=&quot;ECG results&quot;, y=&quot;Frequency&quot;) +
  scale_fill_manual(values=c(&quot;#ce9642&quot;,&quot;#3b7c70&quot;)) +
  theme_classic()</code></pre>
<p><img src="RHandbook_10_files/figure-html/myo_bar-1.png" alt="" width="672" /></p>
<p>Just like with the goodness of fit test, to run a Chi-square test on
a contingency table, we have to convert our data to a table showing the
frequency of individuals in each of the categories. In this case, our
table will have two columns (positive and negative ECG tests) and two
rows(heart attack or no heart attack). We will once again make two
table: one showing the counts and the other showing the proportions in
each category.</p>
<pre class="r"><code>myo_table &lt;- table(cardiac$newMI,cardiac$posECG)
myo_prop &lt;- prop.table(myo_table)</code></pre>
<p>Now that we have our table with the counts, we can run our Chi-square
test to see if patients who tested positive for heart stress were more
likely to have a heart attack within the next year than patients who
tested negative for heart stress. This time, we do not have to provide
the expected frequencies because we are testing whether the frequency of
heart attacks differs between the two test results, NOT if the frequency
matches some specific expected frequency.</p>
<pre class="r"><code>myo_test &lt;- chisq.test(myo_table)</code></pre>
<pre><code>## Warning in chisq.test(myo_table): Chi-squared approximation may be incorrect</code></pre>
<pre class="r"><code>myo_test</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  myo_table
## X-squared = 0.0013321, df = 1, p-value = 0.9709</code></pre>
<p>Notice when you run this test, you get a warning saying that the
“Chi-square approximation may be incorrect”. This is because you have a
low sample size in one of your groups, so your count data might not be
roughly normal. We can deal with this easily by adding an argument to
our test. This argument will tell R to use simulations to estimate the
p-value, which can be a better approach when you have low expected
values for some groups.</p>
<pre class="r"><code>myo_test &lt;- chisq.test(myo_table, simulate.p.value = TRUE)
myo_test</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 2000
##  replicates)
## 
## data:  myo_table
## X-squared = 0.10722, df = NA, p-value = 0.8036</code></pre>
<p>Based on the test result, what do you conclude about whether the
dobutamine stress test was a good predictor of heart attack risk?</p>
<p>To help interpret the data (e.g., see which observed values might be
higher or lower than expected), we can again look at the observed and
expected frequencies in each category.</p>
<pre class="r"><code>myo_test$observed</code></pre>
<pre><code>##         
##          neg pos
##   new_MI  25   3
##   no_MI  462  68</code></pre>
<pre class="r"><code>myo_test$expected</code></pre>
<pre><code>##         
##                neg       pos
##   new_MI  24.43728  3.562724
##   no_MI  462.56272 67.437276</code></pre>
<p>In this case, as expected from our p-value, the observed and expected
frequencies match very closely.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
