<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Emily Schultz" />

<meta name="date" content="2026-01-04" />

<title>R Handbook Lesson 7</title>

<script src="site_libs/header-attrs-2.30/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ESS 3500 R Handbook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lessons
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RHandbook_1.html">Lesson 1: Software installation</a>
    </li>
    <li>
      <a href="RHandbook_2.html">Lesson 2: Data management</a>
    </li>
    <li>
      <a href="RHandbook_3.html">Lesson 3: R basics</a>
    </li>
    <li>
      <a href="RHandbook_4.html">Lesson 4: Data frames</a>
    </li>
    <li>
      <a href="RHandbook_5.html">Lesson 5: Data visualization</a>
    </li>
    <li>
      <a href="RHandbook_6.html">Lesson 6: Graphing in R</a>
    </li>
    <li>
      <a href="RHandbook_7.html">Lesson 7: Hypothesis testing</a>
    </li>
    <li>
      <a href="RHandbook_8.html">Lesson 8: Probability</a>
    </li>
    <li>
      <a href="RHandbook_9.html">Lesson 9: Probability distributions</a>
    </li>
    <li>
      <a href="RHandbook_10.html">Lesson 10: Chi-square tests</a>
    </li>
    <li>
      <a href="RHandbook_11.html">Lesson 11: T-tests</a>
    </li>
    <li>
      <a href="RHandbook_12.html">Lesson 12: Assumptions and transformations</a>
    </li>
    <li>
      <a href="RHandbook_13.html">Lesson 13: ANOVA</a>
    </li>
    <li>
      <a href="RHandbook_14.html">Lesson 14: Correlations and Regressions</a>
    </li>
    <li>
      <a href="RHandbook_15.html">Lesson 15: Multiple predictors</a>
    </li>
  </ul>
</li>
<li>
  <a href="RHandbook_templates.html">R Templates</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">R Handbook Lesson 7</h1>
<h4 class="author">Emily Schultz</h4>
<h4 class="date">2026-01-04</h4>

</div>


<div id="lesson-7-introduction-to-statistics-and-hypothesis-testing"
class="section level1">
<h1>Lesson 7: Introduction to Statistics and Hypothesis Testing</h1>
<div id="why-do-we-use-statistics" class="section level2">
<h2>Why do we use statistics?</h2>
<p>Consider the following two scenarios:</p>
<ol style="list-style-type: decimal">
<li><p>You are restoring a small forest site, and you choose to use a
prescribed burn to reduce the density of invasive trees at your site. To
determine if your prescribed burn was effective, you count all of the
invasive trees at your site before and after the burn and find that
there were fewer invasive trees after the burn.</p></li>
<li><p>You are testing whether prescribed burns are a generally
effective method for controlling a specific species of invasive tree.
You set up five controlled burn plots in different locations and count
the number of invasive trees in each location before and after the
burn.</p></li>
</ol>
<p>In the first scenario, you have all of the information you need to
definitely answer your question. You wanted to compare the density of
invasive trees before and after the burn <strong>at your specific
site</strong>. You were able to measure every invasive tree at your
site, so when you compared the counts, you could conclude with certainty
that there were fewer trees after the burn, no statistical tests
necessary.</p>
<p>In the second scenario, however, you are using a small group (your
sample group) that you were able to measure to draw conclusions about a
much larger group - all invasive populations of the tree species (your
statistical population). This is the scenario that requires statistical
testing. We cannot guarantee that our sample plots are representative of
all populations of the invasive tree that we want to draw conclusions
about, so even if we see a pattern in our data we cannot definitely say
that we would see the same pattern in the full group. Statistical tests
address this problem by giving us a measure of how confident we can be
that any patterns in our data are true patterns we would see if we were
able to measure the whole population. It allows us to draw a
probabilistic conclusion, if not a certain one.</p>
</div>
<div id="statistical-hypotheses" class="section level2">
<h2>Statistical hypotheses</h2>
<p>When we use statistical tests to draw conclusions about a larger
population based on our sample data, our goal is to test specific
hypotheses about our data. These statistical hypotheses, or models, are
slightly different than biological hypotheses we might develop for our
research questions. They do not focus on mechanistic explanations for
patterns we observe. Instead, they focus on the patterns, such as
relationships between variables, that we might observe in our data
themselves.</p>
<p>As statistical practitioners, there are two general types of
hypotheses that we develop and test: the null hypothesis and the
alternative hypothesis.</p>
<p>The null hypothesis is the hypothesis of no effect. The specific type
of effect we refer to depends on our question and the type of test we
are using, but generally speaking, in this class, we will be testing for
the effect of our independent variable on the mean of our dependent
variable. Our null hypothesis would therefore be that our independent
variable has no effect on the mean of our dependent variable.</p>
<p>The alternative hypothesis, as you can probably guess, is the
opposite. It states that our independent variable does have an effect on
the mean of our dependent variable. Sometimes we might state the
direction of the effect in the alternative hypothesis. For example, if a
medical researcher were testing a new drug for reducing cholesterol,
they might only be interested in a positive effect of that drug. In that
case, the alternative would be that the drug lowers cholesterol.
However, most often in ecology, we don’t specify the direction of the
effect, to allow for the possibility that the effect could be in either
direction. If you have more than one independent variable, you can have
more than one alternative hypothesis.</p>
<p>Some examples of null and alternative hypotheses are shown below,
along with figures showing what the data might look like if each
hypothesis were true. (Note that the figures for the alternative
hypothesis show just one example of what the effect might look like -
the effect could vary in strength or direction.)</p>
<div id="example-1" class="section level3">
<h3>Example 1</h3>
<ul>
<li>Question: Does cougar presence affect the density of cottonwood
trees?</li>
<li>Null hypothesis: Cougar density has no effect on the mean density of
cottonwood trees.</li>
<li>Alternative hypothesis: Cougar density affects the mean density of
cottonwood trees.</li>
</ul>
<div class="float">
<img src="ttestmodel.png"
alt="Two density plots of tree density, one (the null model) showing identical distributions of tree densities in the presence and absence of cougars, and the other (the altnerative model) showing a higher mean tree density in the presence of cougars." />
<div class="figcaption">Two density plots of tree density, one (the null
model) showing identical distributions of tree densities in the presence
and absence of cougars, and the other (the altnerative model) showing a
higher mean tree density in the presence of cougars.</div>
</div>
<p><br></p>
</div>
<div id="example-2" class="section level3">
<h3>Example 2</h3>
<ul>
<li>Question: Does soil nitrogen affect the diversity of mycorrhizal
fungi communities?</li>
<li>Null hypothesis: Soil nitrogen has no effect on the mean diversity
of mycorrhizal fungi communities.</li>
<li>Alternative hypothesis: Soil nitrogen affects the mean diversity of
mycorrhizal fungi communities.</li>
</ul>
<div class="float">
<img src="regressionmodel.png"
alt="Two scatterplots of soil nitrogen versus mycorrhizae density, one (the null model) showing no effect of nitrogen on diversity, and the other (the altnerative model) showing a positive effect of nitrogen on density." />
<div class="figcaption">Two scatterplots of soil nitrogen versus
mycorrhizae density, one (the null model) showing no effect of nitrogen
on diversity, and the other (the altnerative model) showing a positive
effect of nitrogen on density.</div>
</div>
<p><br></p>
</div>
</div>
<div
id="what-information-is-important-for-determining-statistical-significance"
class="section level2">
<h2>What information is important for determining statistical
significance?</h2>
<p>When we run statistical tests, we are essentially deciding which of
our hypotheses, the null or alternative, is best for explaining our
data. If our alternative hypothesis comes out as the winner, then we
would say that our independent variable has a statistically significant
effect on our dependent variable.</p>
<p>The way this is done depends of the type of statistical approach we
use (see <a href="#types-of-statistics">Types of statistics</a> below).
However, in one way or another, the different approaches incorporate
similar information. Earlier in the semester, you inferred the pieces of
information that are important for determining statistical significance,
but we will review them here:</p>
<ol style="list-style-type: decimal">
<li><p>Signal: The signal is the true pattern or relationship between
your variables. This could be the difference in the mean of two groups
or the slope of the relationship between two variables, for example. The
stronger the signal, the more confident we can be that it is a true
pattern that we would observe if we were able to sample the full
population.</p></li>
<li><p>Noise: The noise is the leftover variation in the data. This
variation can come from observation error or process error (true
variation caused by variables that are not part of the analysis). Either
way, this variation can obscure the patterns in the data. The greater
the noise, the less confident we can be that the pattern in our data
reflects a true pattern in the full population.</p></li>
<li><p>Sample size: Just like flipping a coin 1000 times is more likely
to result in a head to tail ratio that is close to the expected 50:50
ratio than flipping a coin only 10 times, if we have a larger sample
size, we can be more confident that any patterns in our data are real
and not simply the result of sampling error.</p></li>
</ol>
<p>All together, if we have a high signal and sample size, and low
noise, it is more likely that patterns in the data will be statistically
significant.</p>
</div>
<div id="types-of-statistics" class="section level2">
<h2>Types of statistics</h2>
<p>There are three main statistical approaches that are used to test
hypotheses: classical frequentist approaches, information-criterion
approaches, and Bayesian approaches. Some people have philosophical
reasons for choosing one approach over the other, but in many cases, you
will likely draw the same conclusion no matter which approach you use.
However, each approach has it’s practical advantages that can make it
more effective for particular types of questions.</p>
<p>In this class, we will cover classical frequentist statistics and
maximum likelihood statistics. Classical frequentist statistics have
been widely used for a long time, they are the most commonly taught
statistics in introductory statistics courses (so people will expect you
to understand them if you have taken a class like this one), and they
are the type of statistics you have most likely been exposed to if you
have used statistical tests in the past. Information-criterion
approaches have become more widely-used in ecology, and many of you will
likely use these types of statistics in the future if you continue in
ecology, so I want to expose you to them as well. Although we won’t
cover Bayesian statistics in this class, I still briefly introduce this
approach below because the use of Bayesian statistics is growing rapidly
in ecology, so it’s good to at least be aware of it because you will
likely encounter them in the future.</p>
<div id="classical-frequentist-statistics" class="section level3">
<h3>Classical frequentist statistics</h3>
<p>If you have ever run a statistical test (t-test, ANOVA, regression,
Chi-square test) that resulted in a p-value, you have used classical
frequentist statistics. We will explore the method in detail when we
work through individual types of tests, but I will present the general
approach here.</p>
<p>Classical frequentist statistics probably best fit the definition of
“hypothesis testing”. Specifically, they work by testing the null
hypothesis. The logic behind these types of tests works as follows:</p>
<ol style="list-style-type: decimal">
<li>Assume the null hypothesis is true (i.e., if you were to observe the
full population, there would be no relationship between your
variables).</li>
<li>Imagine that you repeat your experiment over and over again, drawing
a sample of the same size from the full population.</li>
<li>Calculate the probability that, in these repeated experiments, you
would observe a pattern as strong as the one in your data, or stronger,
assuming the null hypothesis is true. This probability is known as the
p-value.</li>
<li>If there is a very low probability of observing a pattern as strong
as the pattern in your data if the null hypothesis is true, then we
would conclude that the null hypothesis is not a good explanation for
our data, so we would reject it. By convention, we typically reject the
null hypothesis if p &lt; 0.05, in other words, if there is a less than
5% chance of observing a pattern as strong, or stronger than ours, if
the null hypothesis is true. If we reject the null hypothesis, we would
then say that there is a statistically significant effect of our
independent variable on our dependent variable.</li>
</ol>
<p>Note that with a p-value of 0.05, there is still a chance that the
null hypothesis is true. We would therefore expect that 5% of the time
(or 1 in 20 analyses), we would reject the null hypothesis even though
it is actually correct. This type of error, concluding there is an
effect when in reality there isn’t, is known as type I error (false
positive). However, there will be many other times when we fail to
reject the null hypothesis (if p &gt; 0.05), even when there is a real
effect. This is known a type II error (false negative). If we wanted to
reduce the type I error rate, we could use a lower threshold p-value. If
we want to reduce the type II error rate, it is best to increase the
sample size and/or reduce variation to give us more power to detect
effects (as per our discussions on experimental design).</p>
<p>Classical frequentist statistics are most useful when you have a
specific null hypothesis that you want to test, rather than multiple
hypothesis that you are trying to compare. They are also a very
well-established and widely-understood approach.</p>
</div>
<div id="information-criterion-statistics" class="section level3">
<h3>Information-criterion statistics</h3>
<p>Information-criterion statistics have some mathematical similarities
to classical frequentist statistics, but rather than focusing on testing
a single hypothesis (the null hypothesis), information criterion
statistics work by comparing multiple hypotheses to each other. On the
simple end, this could be comparing the null hypothesis to a single
alternative hypothesis, but if you have multiple hypothesis, you can
compare those hypotheses to each other as well. Because of this,
information-criterion statistics are often better suited for questions
involving multiple independent variables or different model structures.
This is one reason they have become popular in ecology. We are often
working in messy, uncontrolled systems in which many variable effect our
outcome of interest, and we want to know which variables are most
important for explaining the patterns we see in that outcome.</p>
<p>The logic behind information-criterion statistics is, in my opinion,
less convoluted than the logic behind p-values. Information-criterion
statistics work by testing each of your hypotheses one at a time and
calculating the probability of observing your specific data set,
assuming that hypothesis is true. This probability is known as the
likelihood value. The higher the likelihood value, the better the
hypothesis is for explaining your data. These statistics also penalize
for the complexity of the model (because a more complex model will
always result in at least a slightly higher probability), so ultimately
the model deemed as best is the simplest model that has the highest
probability of leading to your data.</p>
<p>Because this type of statistics considers all of your hypotheses, not
just the null hypothesis, it can be used to rank your hypotheses from
best to worst. However, you can also use a threshold (similar to the p
&lt; 0.05 threshold) to select one model as the best. Usually if there
is a difference in information criterion value of two or more between
two models, the model with the <strong>lower</strong> value is selected
as the best (lower information criterion means higher likelihood and
lower complexity of the model). If your alternative model comes out on
top, you could then conclude that your independent variable has a
significant effect on your dependent variable.</p>
</div>
<div id="bayesian-statistics" class="section level3">
<h3>Bayesian statistics</h3>
<p>An oddity of both classical frequentist and information criterion
statistics is that, even though we are trying to draw conclusions about
our hypothesis, the probabilities we calculate are actually
probabilities about the data. Given a particular hypothesis, what is the
probability of observing data like ours? This can make the
interpretation somewhat unintuitive because if we are trying to assess a
hypothesis, intuitively we should be calculating the probability that
the hypothesis is correct.</p>
<p>That is exactly what Bayesian statistics does for us. Bayesian
approaches use a likelihood calculation, just like information-criterion
approaches, but then apply a mathematical trick, known as Bayes Theorem
(which we will cover in one of our upcoming probability lessons), to
estimate the probability that a particular hypothesis is true. This
makes the interpretation of Bayesian statistics more intuitive. However,
it makes the implementation a lot more computationally intense. In fact,
although the theory behind Bayesian statistics is old, they have only
become practical to implement with modern computers.</p>
<p>In addition to being more intuitive to interpret, Bayesian statistics
have a number of other benefits that are useful in the context of
ecology. One of the biggest strengths of Bayesian statistics is it’s
ability to handle variation, including separating out different sources
of variation, such as observation error and process variation. It is
also useful in forecasting (making predictions about the future) due to
the ease of incorporating both past information and new data.</p>
<p>Despite their differences, all three approaches are rooted in
probability. In the rest of this module, we will dig into some basics of
probability that underlie these tests.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
