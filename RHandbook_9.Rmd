---
title: "ESS 3500 R Handbook, Lesson11"
author: "Emily Schultz"
date: "2023-02-15"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

## Lesson 11: Correlations and regressions in R
In this lesson, you will tackle two types of tests that can be used when you have two continuous variables: correlations and regressions. Correlations are typically used when we think there might be a relationship between the two variables, but not a causal relationship (i.e., you don't have clear independent and dependent variables). A regression is used when we hypothesize that there is a causal relationship between our variables or if we want to be able to use one variable to predict the value of the other, even if there isn't a causal relationship. 

For this lesson, we will work with a summarized version of the plant cover data set that we worked with once before (comparing cover of native and invasive plants), but this time we will look at some climate variables as possible predictor of native plant cover.


### 11.2 Linear regression
Now we will test the effect of climate variables on the plant cover. In this case, it would make sense for there to be a causal relationship between the climate variables and plant cover, so we will use a linear regression.

Good news! You don't have to learn any new code for this!

#### Building and visualizing the models
To keep things simpler, we will just test the effects of the two temperature variables on the total plant cover. For now, we'll build three models: the null model, an alternative model for mean temperature, and an alternative model for maximum temperature.

```{r plant_lm}
plant_null <- lm(tot_cover ~ 1, plant)
plant_mean <- lm(tot_cover ~ Mean_tempC, plant)
plant_max <- lm(tot_cover ~ Max_tempC, plant)
```

To view the output of your models, type the name of each model. Just like when we have worked with this model before, the null model will have just an intercept, and the alternative model will have a intercept and a slope term for the effect of either mean or maximum temperature on plant cover.

Now, let's create graphs to visualize our data. Because we have two continuous variables, a scatterplot is a good option. We will also include a best fit line based on our alternative model. We'll make two graphs, one for each of our independent variables. If you haven't already, load the `tidyverse` package first.

```{r plant_scatter}
library(ggplot2)
ggplot(plant, aes(x=Mean_tempC, y=tot_cover)) +
  geom_point() +
  geom_smooth(method="lm")+
  labs(x="Mean temperature (C)", y="Total plant cover") +
  theme_classic()

ggplot(plant, aes(x=Max_tempC, y=tot_cover)) +
  geom_point() +
  geom_smooth(method="lm")+
  labs(x="Max temperature (C)", y="Total plant cover") +
  theme_classic()
```

#### Classical Frequentist Approach
Let's again start by analyzing the models using a frequentist approach. We don't actually need to run any additional tests for this. We can just look at some additional output from the models we already ran.

To view the additional output, use the `summary` function.

```{r lm_out}
summary(plant_mean)
summary(plant_max)
```

When you view the output, you will see a number of things. First, you will be able to see the the formula you used to build the models. Then you will see some information on the distribution of the residuals (the leftover variation not explained by your model). Next, you will see the coefficients from your model, along with standard error of the estimates. The coefficients section will also show you t-values and p-values for each coefficient. These are one-sample t-tests comparing the value of the coefficient to zero.

The information we really want for our linear regression test is down at the very bottom. In the final section, you will see some R-squared values. These are a measure of how much variation in your dependent variable is explained by your independent variable (we will talk about this more in class). Below that, you will see the output of the linear regression test. First is the F-statistic (the same statistic that was calculated for the ANOVA). Then you will see the p-value. Based on these values, would your reject or accept the null hypothesis? What does this tell you about the effect of temperature on plant growth?

#### Likelihood-based approach
Next we will use a likelihood-based approach to test the same question. Once again, the approach is the same as what you used for the t-test and ANOVA, using the `AIC` function to compare the two models.

```{r cone_aic}
AIC(plant_null, plant_mean, plant_max)
```

Based on this output, what would you conclude about the effect of mean and maximum temperature on plant growth?

### 11.3 Multiple regression
In the previous section, we ran two regressions: one for each of our two models. However, we have discussed the problems with running multiple tests on the same data set. To address this, we can instead run a multiple regression, which is essentially the regression version of the two-way ANOVA. You can include multiple predictors in the same model, but this time, the predictor models are both continuous rather than categorical. We already built our null model and our models with only one predictor, now lets build our two models with both predictors (one without and one with interactions).

```{r mult_models}
plant_both <- lm(tot_cover ~ Mean_tempC + Max_tempC, plant)
plant_int <- lm(tot_cover ~ Mean_tempC * Max_tempC, plant)
```

#### Classical frequentist approach

For the classical frequentist approach, all we have to do is take a look at the output of our full model (both variables with an interaction), so we can interpret the effects of our predictors.

```{r mult_freq}
summary(plant_int)
```

First, look at the statistics for the overall regression (the last line of the output). Based on this, does at least one of the predictors have a significant effect?

Now look at the p-values from the t-test that compare each slope term to zero (in the Coefficients section). Based on these p-values, which effects (mean temperature, maximum temperature, interaction between mean and maximum temperature) are statistically significant?

#### Maximum likelihood approach

You know the drill here. As usual, which just have to compare the AIC values between all of our models.

```{r mult_AIC}
AIC(plant_null, plant_mean, plant_max, plant_both, plant_int)
```

Based on the AIC values, which model(s) is(are) the best? What does this tell you about which predictor are important for explaining plant cover?

#### Comparison between simple and multiple regression
Look back at the conclusions you drew about the effects of your predictor variables from your single and multiple regressions. Do your conclusions match? Why do you think this is (hint: think about the results of your correlation analysis, where you looked at correlations between the predictor variables)?

We'll discuss this as a class, but think on your own/and or discuss in small groups first.

#### Graphing multiple regressions
It is challenging to visualize two continuous predictor variable at the same time (I think 3D graphs are hard to read). One option is just to make separate scatter plots for each predictor, like we did above for our single regressions, but that does not allow you to see interactions between the two variables. What I often do is convert one of the predictors to a categorical variable (just for the graph, not for the test), and graph both of the predictors at the same time. We'll try that.

First, we will use the `mutate` function to create a new variable that converts maximum temperature to a category. We will just use two categories: high and low. High maximum temperature will be anything above the median maximum temperature from our data set, and low maximum temperature will be anything less than or equal to the median. To to this, we will use the `ifelse` function. This function allows us to set a value for something **if** it matches certain criteria (in this case, if the maximum temperature is greater than the median) and set a different values if it does not match the criteria.

```{r temp_cat}
library(tidyverse)

med <- median(plant$Max_tempC)
plant <- mutate(plant,MaxT_cat = ifelse(Max_tempC > med, "High", "Low"))
```

If you view the data set, you will now see a new variable called "MaxT_cat" that we will use to graph our data. Let's make that graph now. We will use "Mean_tempC" as our x variable, and we will use two different colors to represent our two maximum temperature categories.

```{r temp_scat}
ggplot(data=plant, aes(x=Mean_tempC,y=tot_cover,color=MaxT_cat)) +
  geom_point() +
  geom_smooth(method="lm") +
  scale_color_manual(values=c("#ce9642","#3b7c70")) +
  labs(x="Mean temperature", y="Total plant cover",color="Maximum temperature") +
  theme_classic()
```