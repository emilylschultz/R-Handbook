---
title: "ESS 3500 R Handbook, Lesson 11"
author: "Emily Schultz"
date: "2023-02-15"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lesson 11: Two-sample t-tests
In this lesson, you will practice running your first formal statistical test, a t-test! You will use both a classical frequentist and a likelihood-based approach to run the test.

For this test, we will use the *civu.csv* data file. The civu data set contains data from an experiment designed to test the affects of native insect herbivory on a invasive bull thistles. We will use this data set to answer the following question: does insect herbivory affect the density of thistle plants?

Start off by reading in the civu data set that we have worked with before. Be sure to first set your working directory to the folder with the civu dataset. Then use the `head` function to view the top rows of the data set, as a reminder of what data we have in the data frame.

```{r civu_data}
civu <- read.csv("civu.csv")
head(civu)
civu$herbivory <- as.factor(civu$herbivory)
```

In this data set, the "herbivory" variable is our independent variable and the "density_2006" variable will be our dependent variable. Because the herbivory variable is categorical, we need to convert it to a factor variable, as we have done before:

```{r factor}
civu$herbivory <- as.factor(civu$herbivory)
```

Now we're ready to get started on our analysis!

### 11.1 Visualizing the data
First, we are going to visualize the data. We will use two approaches: a standard boxplot and a density plot that shows the full distributions of our data.

We will use the **ggplot2** package to make the graphs, so load that first:

```{r ggplot}
library(ggplot2)
```

#### Boxplot
We'll begin with a boxplot. The syntax is similar to what you used to make histograms, with a few modifications. Here is the full code, will explanations of the modification below:

```{r boxplot}
ggplot(data = civu, aes(x = herbivory, y = density_2006)) +
  geom_boxplot() +
  labs(x = "Herbivory treatment", y = "Density in 2006") +
  theme_classic()
```

As will the histograms, we start with the *ggplot* function, where we tell R what data and variable we want to use in our graph. The data argument is where we input the name of our data frame, as before, and the variables in the aes function With a boxplot, we are graphing two variables (our independent and dependent variables) because we want to visualize the effect of one variable on another. Therefore, we need to include both an x and y variable in the aes function.

In the next row down, we tell R what type of graph we want to make. Here, we want to make a box plot, so we use the *geom_boxplot* function.

The other two lines are similar to the lines we used when we made histograms: they change the axis labels and some of the aesthetics of the graph.

#### Density plot
Now we will make a density plot that compares the full distributions of thistle density for the two herbivory treatments. Again, the syntax will be similar, but I will walk through the differences below:

```{r density}
ggplot(data=civu, aes(x=density_2006,fill=herbivory))+
  geom_density(aes(y=after_stat(density)),alpha=0.5)+
  scale_fill_manual(values=c("#ce9642","#3b7c70"))+
  labs(x="Thistles Density",y="Frequency")+
  theme_classic()
```

In the first line, where we input the variables we want to graph, we do things a little differenly here. In this type of graph, instead of having our independent variable on the x-axis and our dependent variable on the y-axis, we are plotting the frequency of our thistle density variable, like we did for the histograms, but we will show two separate frequency distributions for our two herbivory treatments, and we will show those distribution in two different colors. Therefore, in our aes function, we use the density_2006 variable and our x variable, and then we include the fill argument to tell R that we want two different fill colors for our graph, based on the herbivory category. This will tell R both that we want two separate distributions and that we want them to be shown with two different fill colors.

In this next line down, we use the geom_density function that we have used before to tell R to make a probability density plot. The first argument (y=after_stat(density))) tells R to calculate the probability density of each thistle density, based on the frequency of the values in our data set. The second argument (alpha=0.5), adjusts the transparency of the colors in the graph. The alpha values can range from 0 to 1, with 0 being fully transparent and 1 being fully opaque.

The final line that is different is the addition of the "scale_fill_manual" function. This line is not required, but it allows you to choose the colors in your graph. If you do not include this line, R will use ggplot's default color palette (be aware: the default color palette is not colorblind-friendly). The values I input in to the function are the hex codes for the colors I wanted to use. You can use either hex codes or the names of the colors. I like to use hex codes because they are more universal. Note that if you manually select your colors, you need to choose one color for each category. Here, we only have two categories: herbivory and no herbivory.

Playing around with colors in R can be fun! If you want to explore different color palettes, I included some information and resources in section 5.4 at the bottom of this lesson.

### 11.2 Classical frequentist approach
Now that we have visualized the data, let's run the t-test itself. We will set up the model using the t-test function: `t.test`. The first argument for this function is the formula for our model, where we tell R our independent and dependent variables. The dependent variable goes on the left and the independent variable goes on the right. The second argument is the name of the data set. Then then final argument "var.equal = TRUE" tells R that we are assuming our two treatments have equal variance.

```{r civu_ttest}
civu_ttest <- t.test(density_2006 ~ herbivory, data = civu, var.equal = TRUE)
```

To view the output, just type the name you gave to your t-test object:

```{r ttest_out}
civu_ttest
```

You should see the following pieces in your output:

1. data: This just tells you what variables went into your model
2. t: This is the value of your test statistic (t)
3. df: This is your degrees of freedom. We will discuss this in more detail later, but it is related to your sample size.
4. p-value: This is the probability of observing your test statistic or a more extreme test statistic, assuming your null model is true.
5. alternative hypothesis: This is a verbal statement of the alternative hypothesis being tested.
6. 95 percent confidence interval: If you repeated this experiment over and over, you would expect that 95% of the time, the difference in the sample means of the two treatment would fall between these two values.
7. sample estimates:  These are the means calculated for your two treatment groups.

Based on this output, would you reject the null hypothesis and tentatively accept the alternative hypothesis?

### 11.3 Likelihood-based approach
Next we will use a likelihood-based approach to test the same question. For this, we will start by building our two linear models, just as we did in the Model Building lesson:

```{r civu_lm}
civu_null <- lm(density_2006 ~ 1, civu)
civu_alt <- lm(density_2006 ~ herbivory, civu)
```

Now, we just need to calculate the Akaike's Information Criterion (AIC) values for the two models. We can do this using the `AIC` function. As the arguments, we just need to list the models we want to compare. We can compare more than two models at once, but for this question, we just have our null model and one alternative model.

```{r aic}
AIC(civu_null, civu_alt)
```

You should see the output automatically appear in a table. The first column lists the model. The second column (df) lists the number of parameters in each model (remember, AIC penalizes for adding parameters). The final column lists the AIC values for each model. The lower the AIC value, the better the model. A difference of 2 or more between the AIC values indicates that one model is significantly better than the other.

Based on this output, which is the better model? Is it significantly better?

Does your conclusion from this approach match your conclusion from the classical frequentist approach?

